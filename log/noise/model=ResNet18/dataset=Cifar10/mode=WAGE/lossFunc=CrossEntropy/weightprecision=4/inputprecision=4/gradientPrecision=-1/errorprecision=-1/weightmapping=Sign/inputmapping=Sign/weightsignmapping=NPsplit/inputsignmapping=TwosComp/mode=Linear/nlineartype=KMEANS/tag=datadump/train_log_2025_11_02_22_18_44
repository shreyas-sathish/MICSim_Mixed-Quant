load config from['./Accuracy/config/resnet18/config_resnet18_mixed_prec.ini']
===============================configurations===============================
Section Network   
	 model                : ResNet18
	 dataset              : Cifar10
	 numclass             : 50
Section Quantization
	 mode                 : WAGE
	 weightprecision      : 4
	 inputprecision       : 4
	 errorprecision       : -1
	 gradientprecision    : -1
	 weightsignmapping    : NPsplit
	 inputsignmapping     : TwosComp
	 weightmapping        : Sign
	 inputmapping         : Sign
	 hardware             : False
	 dumpaveragevalue     : False
	 dumpaveragevalue_path : ./average_files/ResNet18/LSQ4/Device/PCM/
	 layer_quant_config   : [
("conv1", 8, 8, 1.0),
("group1_block1_conv1", 8, 8, 1.0),
("group1_block1_conv2", 8, 8, 1.0),
("group1_block2_conv1", 8, 8, 1.0),
("group1_block2_conv2", 8, 8, 1.0),
("group2_block1_conv1", 8, 8, 1.0),
("group2_block1_conv2", 8, 8, 1.0),
("group2_block2_conv1", 8, 8, 1.0),
("group2_block2_conv2", 8, 8, 1.0),
("group3_block1_conv1", 8, 8, 1.0),
("group3_block1_conv2", 8, 8, 1.0),
("group3_block2_conv1", 8, 8, 1.0),
("group3_block2_conv2", 4, 8, 1.0),
("group4_block1_conv1", 4, 8, 1.0),
("group4_block1_conv2", 4, 8, 1.0),
("group4_block2_conv1", 4, 8, 1.0),
("group4_block2_conv2", 4, 8, 1.0),
("last_layer", 4, 8, 1.0)
]
Section Training  
	 lossfunc             : CrossEntropy
	 optimizer            : QSGD
	 batch_size           : 128
	 learning_rate        : 0.1
	 bn_learning_rate     : 0.1
	 numepoch             : 100
	 decreasing_lr        : 50, 80
	 momentum             : 0.9
	 train_log_interval   : 100
	 val_log_interval     : 1
Section Inference 
	 pretrained           : True
	 savedmodel           : ./saved_model/Resnet18-WAGE-8bit.pth
Section Path      
	 log_dir              : ./log/noise
	 organize             : Network_model,Network_dataset,Quantization_mode,Training_lossFunc,Quantization_weightprecision,Quantization_inputprecision,Quantization_gradientPrecision,Quantization_errorprecision,Quantization_weightmapping,Quantization_inputmapping,Quantization_weightsignmapping,Quantization_inputsignmapping,ADC_mode,ADC_nlineartype
	 tag                  : datadump
Section System    
	 gpu                  : 0
Section CIM       
	 arraysize            : 128
	 cellprecision        : 4
	 cycleprecision       : 1
	 digitref2            : False
	 digitref3            : False
	 withcellvar          : False
Section Device    
	 resmap               : ./Accuracy/src/Component/cell_files/fake_device_ronoff.csv
	 gmincancel           : False
Section ADC       
	 mode                 : Linear
	 type                 : SAR
	 share                : 8
	 dumpdata             : False
	 dumpdatapath         : collected_data
	 std_file             : ./Accuracy/src/Component/ADC_files/Linear_std/test.csv
	 ref_file             : ./Accuracy/src/Component/ADC_files/NLinear/test.csv
	 linear_file          : ./Accuracy/src/Component/ADC_files/Linear/ResNet18/lsq/w4in1/Case3/bit8.csv
	 nlinear_file         : ./Accuracy/src/Component/ADC_files/NLinear/VGG8/WAGE/8/8/Sign/Sign/TwosComp/TwosComp/1/1/128/KMEANS/level_4_old.csv
	 nlineartype          : KMEANS
Section NonIdeal  
	 noiseloc             : None
	 noisetype            : Gaussian
	 noisestd             : 10
	 printstat            : False
	 weightnoise          : 0.0
===============================configurations===============================
ResNet(
  (conv1): QConv2d(
    kernel_size=(7, 7), in_channels=3, out_channels=64, stride=(2, 2), bias=False, quantize_weight=True, quantize_input=False, quantize_error=False
    (quantizer): WAGEQuantizer()
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): QConv2d(
        kernel_size=(3, 3), in_channels=64, out_channels=64, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): QConv2d(
        kernel_size=(3, 3), in_channels=64, out_channels=64, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): QConv2d(
        kernel_size=(3, 3), in_channels=64, out_channels=64, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): QConv2d(
        kernel_size=(3, 3), in_channels=64, out_channels=64, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): QConv2d(
        kernel_size=(3, 3), in_channels=64, out_channels=128, stride=(2, 2), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): QConv2d(
        kernel_size=(3, 3), in_channels=128, out_channels=128, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): QConv2d(
          kernel_size=(1, 1), in_channels=64, out_channels=128, stride=(2, 2), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
          (quantizer): WAGEQuantizer()
        )
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): QConv2d(
        kernel_size=(3, 3), in_channels=128, out_channels=128, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): QConv2d(
        kernel_size=(3, 3), in_channels=128, out_channels=128, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): QConv2d(
        kernel_size=(3, 3), in_channels=128, out_channels=256, stride=(2, 2), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): QConv2d(
        kernel_size=(3, 3), in_channels=256, out_channels=256, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): QConv2d(
          kernel_size=(1, 1), in_channels=128, out_channels=256, stride=(2, 2), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
          (quantizer): WAGEQuantizer()
        )
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): QConv2d(
        kernel_size=(3, 3), in_channels=256, out_channels=256, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): QConv2d(
        kernel_size=(3, 3), in_channels=256, out_channels=256, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): QConv2d(
        kernel_size=(3, 3), in_channels=256, out_channels=512, stride=(2, 2), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): QConv2d(
        kernel_size=(3, 3), in_channels=512, out_channels=512, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): QConv2d(
          kernel_size=(1, 1), in_channels=256, out_channels=512, stride=(2, 2), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
          (quantizer): WAGEQuantizer()
        )
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): QConv2d(
        kernel_size=(3, 3), in_channels=512, out_channels=512, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): QConv2d(
        kernel_size=(3, 3), in_channels=512, out_channels=512, stride=(1, 1), bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
        (quantizer): WAGEQuantizer()
      )
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): QLinear(
    in_features=512, out_features=50 bias=False, quantize_weight=True, quantize_input=True, quantize_error=False
    (quantizer): WAGEQuantizer()
  )
)
Applied layer-wise quantization configuration:
	conv1: scaling=1.0, wbits=8, abits=8
	conv1.quantizer: scaling=1.0, wbits=NA, abits=NA
	layer1.0.conv1: scaling=1.0, wbits=8, abits=8
	layer1.0.conv1.quantizer: scaling=1.0, wbits=NA, abits=NA
	layer1.1.conv1: scaling=1.0, wbits=8, abits=8
	layer1.1.conv1.quantizer: scaling=1.0, wbits=NA, abits=NA
	layer2.0.conv1: scaling=1.0, wbits=8, abits=8
	layer2.0.conv1.quantizer: scaling=1.0, wbits=NA, abits=NA
	layer2.1.conv1: scaling=1.0, wbits=8, abits=8
	layer2.1.conv1.quantizer: scaling=1.0, wbits=NA, abits=NA
	layer3.0.conv1: scaling=1.0, wbits=8, abits=8
	layer3.0.conv1.quantizer: scaling=1.0, wbits=NA, abits=NA
	layer3.1.conv1: scaling=1.0, wbits=8, abits=8
	layer3.1.conv1.quantizer: scaling=1.0, wbits=NA, abits=NA
	layer4.0.conv1: scaling=1.0, wbits=8, abits=8
	layer4.0.conv1.quantizer: scaling=1.0, wbits=NA, abits=NA
	layer4.1.conv1: scaling=1.0, wbits=8, abits=8
	layer4.1.conv1.quantizer: scaling=1.0, wbits=NA, abits=NA
start time: 2025_11_02_22_18_51
===================== testing phase =====================
	Test set: Average loss: 3.9298, Accuracy: 95/10000 (1%)
finish time: 2025_11_02_22_19_01
